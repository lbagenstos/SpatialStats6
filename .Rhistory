# dtm_cleaned <- DocumentTermMatrix(myCorpus)
# tm::inspect(dtm_cleaned)
dtm_cleaned <- tdm_filtered
# dtm_cleaned <- DocumentTermMatrix(myCorpus)
# tm::inspect(dtm_cleaned)
dtm_cleaned <- tdm#_filtered
dtm_cleaned <- t(dtm_cleaned)  #transpose TDM → DTM
dtm_cleaned
m <- dtm_cleaned  # keep as a sparse matrix
dim(m)
rownames(m) <- paste0("Review_", seq_len(nrow(m)))
# m <- as.matrix(dtm_cleaned)
# dim(m)
# colnames(m) <- dtm_cleaned$dimnames$Terms
# rownames(m) <- c("War and Peace", "Crime and Punishment", "Pride and Prejudice", "Tale of Two Cities", "Emma", "Brothers Karamazov", "Jane Eyre",
# "The Great Gatsby", "Wuthering Heights", "Moby Dick", "Call of the Wild", "Frankenstein", "Little Women", "Vanity Fair", "Anna Karenina",
# "Count of Monte Cristo", "Great Expectations", "Les Miserables", "Dracula", "Madame Bovary", "Don Quixote", "Gullivers Travels", "Huckleberry Finn",
# "Picture of Dorian Gray", "White Fang", "A Christmas Carol", "The Idiot", "The Time Machine", "Age of Innocence", "Of Human Bondage", "Paradise Lost",
# "Robinson Crusoe", "Candide", "Last of the Mohicans", "Dead Souls", "Scarlet Letter", "Treasure Island", "Ulysses", "The Trial", "The Three Musketeers", "The Metamorphosis", "Faust", "The Prince", "Leviathan", "David Copperfield", "Notre Dame de Paris", "Utopia")
#sparse-safe term frequency distribution:
cs <- col_sums(m)
hist(cs, breaks = 100)
tab <- table(cs)
wordcloud(
words = names(cs),
freq  = cs,
min.freq = 1000,
max.words = 150
)
# cs <- as.matrix(colSums(m))             #How many times each term appears across all documents (texts)
# rownames(cs) <- dtm_cleaned$dimnames$Terms
#
# hist(cs, breaks=100)                    #Let's look at some histograms/tabulations/word cloud of total term appearance.
# tab <- as.matrix(table(cs))
# wordcloud(myCorpus, min.freq=1000)
variables_to_remove <- cs < 10000
# Subset matrix frame, excluding those variables
m_subset <- m[, !variables_to_remove]
variables_to_remove <- cs < 100 #changed to not get rid of too many
# Subset matrix frame, excluding those variables
m_subset <- m[, !variables_to_remove]
# #Some books are longer, others are shorter. Let's divide the frequencies by the total number of words (after processing) in each book.
# m_fin <- m_subset/rowSums(m)
# Normalize by document length
doc_lengths <- row_sums(m)
m_fin <- m_subset / doc_lengths
# Let's scale (normalize) each of the variables (relative frequency)
m_scale <- scale(as.matrix(m_fin))
# #Scree Plot
# wss <- (nrow(m_scale)-1)*sum(apply(m_scale,2,var))
# for (i in 2:20) wss[i] <- sum(kmeans(m_scale,
#                                      centers=i)$withinss)
# plot(1:20, wss, type="b", xlab="Number of Clusters",
#      ylab="Within groups sum of squares")
#
# #NbClust approach
# set.seed(1234)
# nc <- NbClust(m_scale, min.nc=2, max.nc=15, method="kmeans", index="all")
# #table(nc$Best.n[1,])
# par(mfrow=c(1,1))
# barplot(table(nc$Best.n[1,]),
#         xlab="Numer of Clusters", ylab="Number of Criteria",
#         main="Number of Clusters Chosen by 26 Criteria")
set.seed(123)
max_k <- 15
wss <- numeric(max_k)
for (k in 1:max_k) {
wss[k] <- kmeans(m_scale, centers = k, nstart = 20)$tot.withinss
}
# #Scree Plot
# wss <- (nrow(m_scale)-1)*sum(apply(m_scale,2,var))
# for (i in 2:20) wss[i] <- sum(kmeans(m_scale,
#                                      centers=i)$withinss)
# plot(1:20, wss, type="b", xlab="Number of Clusters",
#      ylab="Within groups sum of squares")
#
# #NbClust approach
# set.seed(1234)
# nc <- NbClust(m_scale, min.nc=2, max.nc=15, method="kmeans", index="all")
# #table(nc$Best.n[1,])
# par(mfrow=c(1,1))
# barplot(table(nc$Best.n[1,]),
#         xlab="Numer of Clusters", ylab="Number of Criteria",
#         main="Number of Clusters Chosen by 26 Criteria")
set.seed(123)
max_k <- 15
wss <- numeric(max_k)
for (k in 1:max_k) {
wss[k] <- kmeans(m_scale, centers = k, nstart = 20)$tot.withinss
}
IMDB_reviews <- read.csv("IMDB_Dataset.csv", nrows=200)
glimpse(IMDB_reviews)
# create a corpus from the "review" text column
myCorpus <- VCorpus(VectorSource(IMDB_reviews$review))
# check example
cat(content(myCorpus[[1]]))
# convert  to lowercase
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
#     Defining the toSpace function
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#     Defining the remApostrophe function
remApostrophe <- content_transformer(function(x,pattern) gsub(pattern, "", x))
#     Removing special characters
myCorpus <- tm_map(myCorpus, toSpace, "@")
myCorpus <- tm_map(myCorpus, toSpace, "/")
myCorpus <- tm_map(myCorpus, toSpace, "]")
myCorpus <- tm_map(myCorpus, toSpace, "$")
myCorpus <- tm_map(myCorpus, toSpace, "—")
myCorpus <- tm_map(myCorpus, toSpace, "‐")
myCorpus <- tm_map(myCorpus, toSpace, "”")
myCorpus <- tm_map(myCorpus, toSpace, "‘")
myCorpus <- tm_map(myCorpus, toSpace, "“")
myCorpus <- tm_map(myCorpus, toSpace, "‘")
myCorpus <- tm_map(myCorpus, remApostrophe, "’")
myCorpus <- tm_map(myCorpus, remApostrophe, "«")
myCorpus <- tm_map(myCorpus, remApostrophe, "»")
cat(content(myCorpus[[1]]))
myCorpus <- tm::tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removePunctuation)
#remove HTML line breaks ("br")
myCorpus <- tm_map(myCorpus, toSpace, "br")
myCorpus <- tm_map(myCorpus, toSpace, "<br\\s*/?>")
cat(content(myCorpus[[1]]), sep = "\n")
stopwords("english")
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
cat(content(myCorpus[[1]]), sep = "\n")
tdm <- TermDocumentMatrix(myCorpus)
tdm
m <- sort(row_sums(tdm), decreasing = TRUE)
head(m, 20)
dictionary <- as.character(words::words$word)
# row_names <- names(m)
# in_dictionary <- row_names %in% dictionary
# remove <- row_names[!in_dictionary]
#Since the data are so large, if we try to remove all words at once, we get an error. So we will remove them in chunks of 1000.
# num_observations <- as.numeric(length(remove))  # Total number of observations
# chunk_size <- 1000                              # Number of observations to display at a time
# for (i in seq(1, num_observations, chunk_size)) {
#   start <- i
#   end <- i + chunk_size - 1
#   end <- ifelse(end > num_observations, num_observations, end)
#   myCorpus <- tm_map(myCorpus, removeWords, remove[start:end])
# }
#instead, trying:
keep_terms <- names(term_freq) %in% dictionary
# cat(content(myCorpus[[1]])[980:1000], sep = "\n")
dropped_terms <- setdiff(
Terms(tdm),
Terms(tdm_filtered)
)
# dtm_cleaned <- DocumentTermMatrix(myCorpus)
# tm::inspect(dtm_cleaned)
dtm_cleaned <- tdm#_filtered
dtm_cleaned <- t(dtm_cleaned)  #transpose TDM → DTM
dtm_cleaned
m <- dtm_cleaned  # keep as a sparse matrix
dim(m)
rownames(m) <- paste0("Review_", seq_len(nrow(m)))
# m <- as.matrix(dtm_cleaned)
# dim(m)
# colnames(m) <- dtm_cleaned$dimnames$Terms
# rownames(m) <- c("War and Peace", "Crime and Punishment", "Pride and Prejudice", "Tale of Two Cities", "Emma", "Brothers Karamazov", "Jane Eyre",
# "The Great Gatsby", "Wuthering Heights", "Moby Dick", "Call of the Wild", "Frankenstein", "Little Women", "Vanity Fair", "Anna Karenina",
# "Count of Monte Cristo", "Great Expectations", "Les Miserables", "Dracula", "Madame Bovary", "Don Quixote", "Gullivers Travels", "Huckleberry Finn",
# "Picture of Dorian Gray", "White Fang", "A Christmas Carol", "The Idiot", "The Time Machine", "Age of Innocence", "Of Human Bondage", "Paradise Lost",
# "Robinson Crusoe", "Candide", "Last of the Mohicans", "Dead Souls", "Scarlet Letter", "Treasure Island", "Ulysses", "The Trial", "The Three Musketeers", "The Metamorphosis", "Faust", "The Prince", "Leviathan", "David Copperfield", "Notre Dame de Paris", "Utopia")
#sparse-safe term frequency distribution:
cs <- col_sums(m)
hist(cs, breaks = 100)
tab <- table(cs)
wordcloud(
words = names(cs),
freq  = cs,
min.freq = 1000,
max.words = 150
)
# cs <- as.matrix(colSums(m))             #How many times each term appears across all documents (texts)
# rownames(cs) <- dtm_cleaned$dimnames$Terms
#
# hist(cs, breaks=100)                    #Let's look at some histograms/tabulations/word cloud of total term appearance.
# tab <- as.matrix(table(cs))
# wordcloud(myCorpus, min.freq=1000)
variables_to_remove <- cs < 100 #changed to not get rid of too many
# Subset matrix frame, excluding those variables
m_subset <- m[, !variables_to_remove]
# #Some books are longer, others are shorter. Let's divide the frequencies by the total number of words (after processing) in each book.
# m_fin <- m_subset/rowSums(m)
# Normalize by document length
doc_lengths <- row_sums(m)
m_fin <- m_subset / doc_lengths
# Let's scale (normalize) each of the variables (relative frequency)
m_scale <- scale(as.matrix(m_fin))
# #Scree Plot
# wss <- (nrow(m_scale)-1)*sum(apply(m_scale,2,var))
# for (i in 2:20) wss[i] <- sum(kmeans(m_scale,
#                                      centers=i)$withinss)
# plot(1:20, wss, type="b", xlab="Number of Clusters",
#      ylab="Within groups sum of squares")
#
# #NbClust approach
# set.seed(1234)
# nc <- NbClust(m_scale, min.nc=2, max.nc=15, method="kmeans", index="all")
# #table(nc$Best.n[1,])
# par(mfrow=c(1,1))
# barplot(table(nc$Best.n[1,]),
#         xlab="Numer of Clusters", ylab="Number of Criteria",
#         main="Number of Clusters Chosen by 26 Criteria")
set.seed(123)
max_k <- 15
wss <- numeric(max_k)
for (k in 1:max_k) {
wss[k] <- kmeans(m_scale, centers = k, nstart = 20)$tot.withinss
}
plot(
1:max_k, wss,
type = "b",
pch = 19,
xlab = "Number of clusters (k)",
ylab = "Total within-cluster sum of squares",
main = "Elbow (Scree) Plot"
)
set.seed(1234)
nc <- NbClust(m_scale, min.nc=2, max.nc=15, method="kmeans", index="all")
table(nc$Best.n[1,])
par(mfrow=c(1,1))
barplot(table(nc$Best.n[1,]),
xlab="Numer of Clusters", ylab="Number of Criteria",
main="Number of Clusters Chosen by 26 Criteria")
k_means_results <- kmeans(m_scale, 2, 30)
k_means_results$cluster
k_means_results$size
word_totals_by_cluster <- round(aggregate(m_subset, by=list(cluster=k_means_results$cluster), sum),1)
library(Matrix)
m_sparse <- as(m_subset, "dgCMatrix")
m_mat <- as.matrix(m_subset)
m_sparse <- Matrix(m_mat, sparse = TRUE)
m_sparse <- as(m_subset, "dgCMatrix")
nrow(m_subset)
length(k_means_results$cluster)
word_totals_by_cluster <- round(
rowsum(
as.matrix(m_subset$v),
k_means_results$cluster
),
1
)
word_totals_by_cluster <- round(aggregate(m_subset, by=list(cluster=k_means_results$cluster), sum),1)
word_totals_by_cluster <- rowsum(
as.matrix(m_subset),
k_means_results$cluster
)
#Let's plot the results!
#Decrease font size and rotate x-axis labels vertically
par(cex.axis = 0.7)  # Adjust the font size
par(las = 2)        # Rotate labels vertically
barplot(as.matrix(word_totals_by_cluster[-1]),
beside = TRUE,
col = c("blue", "green"),
legend.text = TRUE,
args.legend = list(x = "topright"))
# Add labels to the x-axis and y-axis. Here, the first cluster is in blue and the second one is in green.
title(xlab = "Cluster")
barplot(as.matrix(word_totals_by_cluster[-1]),
beside = TRUE,
col = c("blue", "green"),
legend.text = TRUE,
args.legend = list(x = "topright"))
# Add labels to the x-axis and y-axis. Here, the first cluster is in blue and the second one is in green.
title(xlab = "Cluster")
title(ylab = "Sum")
# Add a title to the plot
title(main = "Bar Plot of Sums by Group")
barplot(as.matrix(word_totals_by_cluster[-1]),
beside = TRUE,
col = c("blue", "green"),
legend.text = TRUE,
args.legend = list(x = "topright"))
title(
main = "Bar Plot of Sums by Cluster",
xlab = "Words",
ylab = "Sum"
)
dim(word_totals_by_cluster)
word_totals_by_cluster <- as.matrix(
rowsum_simple_triplet_matrix(
dtm$v,
k_means_results$cluster
)
)
library(slam)
word_totals_by_cluster <- as.matrix(
rowsum_simple_triplet_matrix(
dtm$v,
k_means_results$cluster
)
)
word_totals_by_cluster <- as.matrix(
rowsum_simple_triplet_matrix(
dtm$v,
k_means_results$cluster
)
)
word_totals_by_cluster_sparse <- rollup(
dtm,                   # your DocumentTermMatrix
margin = 1,            # 1 = rows/documents
index = k_means_results$cluster,
FUN = sum
)
word_totals_by_cluster_sparse <- rollup(
dtm_cleaned,                   # your DocumentTermMatrix
margin = 1,            # 1 = rows/documents
index = k_means_results$cluster,
FUN = sum
)
word_totals_by_cluster_sparse <- rollup(
dtm_cleaned,                   # your DocumentTermMatrix
MARGIN = 1,            # 1 = rows/documents
index = k_means_results$cluster,
FUN = sum
)
#Let's plot the results!
#Decrease font size and rotate x-axis labels vertically
par(cex.axis = 0.7)  # Adjust the font size
par(las = 2)        # Rotate labels vertically
barplot(as.matrix(word_totals_by_cluster[-1]),
beside = TRUE,
col = c("blue", "green"),
legend.text = TRUE,
args.legend = list(x = "topright"))
title(
main = "Bar Plot of Sums by Cluster",
xlab = "Words",
ylab = "Sum"
)
top_terms <- names(sort(col_sums(word_totals_by_cluster_sparse), decreasing = TRUE))[1:10]
plot_matrix <- as.matrix(word_totals_by_cluster_sparse[, top_terms])
rownames(plot_matrix) <- paste0("Cluster_", 1:nrow(plot_matrix))
par(cex.axis = 0.8, las = 2)
barplot(
t(plot_matrix),
beside = TRUE,
col = c("blue", "green"),
legend.text = rownames(plot_matrix),
args.legend = list(x = "topright"),
main = "Top Words by Cluster",
xlab = "Words",
ylab = "Total Frequency"
)
```{r warning=FALSE, message=FALSE, cache=FALSE}
nrc <- syuzhet::get_sentiment_dictionary(dictionary="nrc")
head(nrc, n=20L)
afinn <- syuzhet::get_sentiment_dictionary(dictionary="afinn")
head(afinn, n=20L)
bing <- syuzhet::get_sentiment_dictionary(dictionary="bing")
head(bing, n=20L)
syuzhet <- syuzhet::get_sentiment_dictionary(dictionary="syuzhet")
get_nrc_sentiment("gorgeous")
Review_1 <- as.data.frame(m[1,])
#added some code to fix error
first_row_matrix <- as.matrix(dtm[1, ])  # select first row
#added some code to fix error
first_row_matrix <- as.matrix(dtm_cleaned[1, ])  # select first row
first_row_df <- data.frame(term = colnames(dtm), count = as.numeric(first_row_matrix[1, ]))
first_row_df <- data.frame(term = colnames(dtm_cleaned), count = as.numeric(first_row_matrix[1, ]))
Review_1 <- data.frame(term = colnames(dtm_cleaned), count = as.numeric(first_row_matrix[1, ]))
Review_1$Term <- as.vector(rownames(War_And_Peace))
Review_1$Term <- as.vector(rownames(Review_1))
colnames(Review_1)[1] = "Term_Frequency"
rownames(Review_1) <- 1:nrow(War_And_Peace)
rownames(Review_1) <- 1:nrow(Review_1)
nrc_sentiment <- get_nrc_sentiment(Review_1$Term)
Review_1_Sentiment <- cbind(Review_1, nrc_sentiment)
# Select the columns to be multiplied (last ten columns)
cols_to_multiply <- names(Review_1_Sentiment)[3:12]
# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Review_1_Sentiment[, cols_to_multiply] <- Review_1_Sentiment[, cols_to_multiply] * Review_1_Sentiment$Term_Frequency
str(Review_1_Sentiment[, cols_to_multiply])
Review_1_Sentiment$Term_Frequency
head(Review_1)
#added some code to fix error
first_row_matrix <- as.matrix(dtm_cleaned[1, ])  # select first row
Review_1 <- data.frame(term = colnames(dtm_cleaned), count = as.numeric(first_row_matrix[1, ]))
Review_1$Term <- as.vector(rownames(Review_1))
colnames(Review_1)[2] = "Term_Frequency"
rownames(Review_1) <- 1:nrow(Review_1)
nrc_sentiment <- get_nrc_sentiment(Review_1$Term)
Review_1_Sentiment <- cbind(Review_1, nrc_sentiment)
# Select the columns to be multiplied (last ten columns)
cols_to_multiply <- names(Review_1_Sentiment)[3:12]
# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Review_1_Sentiment[, cols_to_multiply] <- Review_1_Sentiment[, cols_to_multiply] * Review_1_Sentiment$Term_Frequency
Review_1_Sentiment$Term_Frequency
head(Review_1_Sentiment)
Review_1_Sentiment$Term_Frequency
str(Review_1_Sentiment[, cols_to_multiply])
# Select the columns to be multiplied (last ten columns)
cols_to_multiply <- names(Review_1_Sentiment)[4:12]
# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Review_1_Sentiment[, cols_to_multiply] <- Review_1_Sentiment[, cols_to_multiply] * Review_1_Sentiment$Term_Frequency
str(Review_1_Sentiment[, cols_to_multiply])
Review_1_Sentiment_Total <- t(as.matrix(colSums(Review_1_Sentiment[,-1:-2])))
Review_1_Sentiment_Total <- t(as.matrix(colSums(Review_1_Sentiment[,-1:-2])))
Review_1_Sentiment[, -1:-2] <- lapply(
Review_1_Sentiment[, -1:-2],
function(x) as.numeric(as.character(x))
)
Review_1_Sentiment_Total <- t(as.matrix(colSums(Review_1_Sentiment[,-1:-2])))
barplot(Review_1_Sentiment_Total, las=2, ylab='Count', main='Sentiment Scores')
Review_1_Sentiment_Total <- t(as.matrix(colSums(Review_1_Sentiment[, -1:-2])))
barplot(
Review_1_Sentiment_Total,
las = 2,             # rotate x-axis labels vertically
ylab = 'Count',
main = 'Sentiment Scores',
col = 'skyblue'
)
Review_1_Sentiment[, -1:-2] <- lapply(
Review_1_Sentiment[, -1:-2],
function(x) as.numeric(as.character(x))
)
cols_to_multiply <- names(Review_1_Sentiment)[4:12]
cols_to_multiply
head(Review_1_Sentiment)
# Sum each emotion column
sentiment_totals <- colSums(Review_1_Sentiment[, cols_to_multiply])
barplot(
Review_1_Sentiment_Total,
las = 2,             # rotate x-axis labels vertically
ylab = 'Count',
main = 'Sentiment Scores',
col = 'skyblue'
)
Review_1_Sentiment_Total <- t(as.matrix(colSums(Review_1_Sentiment[,-1:-2])))
Review_1_Sentiment_Total
cols_to_multiply
Review_1_Sentiment$Term_Frequency
# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Review_1_Sentiment[, cols_to_multiply] <- Review_1_Sentiment[, cols_to_multiply] * Review_1_Sentiment$Term_Frequency
Review_1_Sentiment_Total <- t(as.matrix(colSums(Review_1_Sentiment[,-1:-2])))
barplot(Review_1_Sentiment_Total, las=2, ylab='Count', main='Sentiment Scores')
# Select the columns to be multiplied (last ten columns)
cols_to_multiply <- names(Review_1_Sentiment)[3:12]
# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Review_1_Sentiment[, cols_to_multiply] <- Review_1_Sentiment[, cols_to_multiply] * Review_1_Sentiment$Term_Frequency
Review_1_Sentiment_Total <- t(as.matrix(colSums(Review_1_Sentiment[,-1:-2])))
barplot(Review_1_Sentiment_Total, las=2, ylab='Count', main='Sentiment Scores')
Review_1_Sentiment <- cbind(Review_1, nrc_sentiment)
# Select the columns to be multiplied (last ten columns)
cols_to_multiply <- names(Review_1_Sentiment)[3:12]
# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Review_1_Sentiment[, cols_to_multiply] <- Review_1_Sentiment[, cols_to_multiply] * Review_1_Sentiment$Term_Frequency
# Select the columns to be multiplied (last ten columns)
cols_to_multiply <- names(Review_1_Sentiment)[4:12]
# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Review_1_Sentiment[, cols_to_multiply] <- Review_1_Sentiment[, cols_to_multiply] * Review_1_Sentiment$Term_Frequency
Review_1_Sentiment_Total <- t(as.matrix(colSums(Review_1_Sentiment[,-1:-2])))
# Select the columns to be multiplied (last ten columns)
cols_to_multiply <- names(Review_1_Sentiment)[4:12]
# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Review_1_Sentiment[, cols_to_multiply] <- Review_1_Sentiment[, cols_to_multiply] * Review_1_Sentiment$Term_Frequency
# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Review_1_Sentiment[, cols_to_multiply] <- Review_1_Sentiment[, cols_to_multiply] * Review_1_Sentiment$Term_Frequency
Review_1_Sentiment_Total <- t(as.matrix(colSums(Review_1_Sentiment[,-1:-2])))
Review_1_Sentiment
Review_1_Sentiment_Total <- t(as.matrix(colSums(Review_1_Sentiment[,-1:-3])))
barplot(Review_1_Sentiment_Total, las=2, ylab='Count', main='Sentiment Scores')
Review_1$Term <- as.vector(rownames(Review_1))
colnames(Review_1)[2] = "Term_Frequency"
rownames(Review_1) <- 1:nrow(Review_1)
nrc_sentiment <- get_nrc_sentiment(Review_1$Term)
```
Review_1$Term <- as.vector(rownames(Review_1))
colnames(Review_1)[2] = "Term_Frequency"
rownames(Review_1) <- 1:nrow(Review_1)
nrc_sentiment <- get_nrc_sentiment(Review_1$Term)
Review_1_Sentiment <- cbind(Review_1, nrc_sentiment)
Review_1_Sentiment <- cbind(Review_1, nrc_sentiment)
sentiment_cols <- colnames(nrc_sentiment)
Review_1_Sentiment[, sentiment_cols] <- lapply(Review_1_Sentiment[, sentiment_cols], as.numeric)
# Multiply by Term_Frequency
Review_1_Sentiment[, sentiment_cols] <- Review_1_Sentiment[, sentiment_cols] * Review_1_Sentiment$Term_Frequency
sentiment_totals <- colSums(Review_1_Sentiment[, sentiment_cols])
barplot(
sentiment_totals,
las = 2,
ylab = "Weighted Count",
main = "Sentiment Scores",
col = c(rep("red",5), rep("green",5))  # negative emotions red, positive green
)
Review_1$Term
Review_1$Term <- as.vector(rownames(Review_1))
Review_1$Term <- as.vector(rownames(Review_1))
Review_1$Term
#added some code to fix error
first_row_matrix <- as.matrix(dtm_cleaned[1, ])  # select first row
Review_1 <- data.frame(term = colnames(dtm_cleaned), count = as.numeric(first_row_matrix[1, ]))
#Review_1$Term <- as.vector(rownames(Review_1))
colnames(Review_1)[2] = "Term_Frequency"
rownames(Review_1) <- 1:nrow(Review_1)
nrc_sentiment <- get_nrc_sentiment(Review_1$Term)
nrc_sentiment <- get_nrc_sentiment(Review_1$term)
Review_1_Sentiment <- cbind(Review_1, nrc_sentiment)
nrc_sentiment <- get_nrc_sentiment(Review_1$term)
Review_1$term
nrc_sentiment <- get_nrc_sentiment(Review_1$term)
Review_1_Sentiment <- cbind(Review_1, nrc_sentiment)
# Select the columns to be multiplied (last ten columns)
cols_to_multiply <- names(Review_1_Sentiment)[4:12] #I changed from 3 to 4 b/c it was capturing a text column -AM
# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Review_1_Sentiment[, cols_to_multiply] <- Review_1_Sentiment[, cols_to_multiply] * Review_1_Sentiment$Term_Frequency
Review_1_Sentiment_Total <- t(as.matrix(colSums(Review_1_Sentiment[,-1:-3]))) #I changed from -2 to -3 to match above -AM
barplot(Review_1_Sentiment_Total, las=2, ylab='Count', main='Sentiment Scores')
