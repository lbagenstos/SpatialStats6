---
title: "Text Analysis"
author: "Eugene Brusilovskiy"
date: "`r Sys.Date()`"
output: rmdformats::readthedown
---

The aim of this Markdown is to demonstrate the use of various text analysis tools in R, including text clustering, word clouds and sentiment analysis.

## Data Description

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)
```

First, let's load the required `R` libraries.

```{r libraries, message=FALSE, warning=FALSE}
library(wordcloud)
library(text)
library(tm)
library(SnowballC)
library(words)
library(NbClust)
library(stringr)
library(dplyr)
library(syuzhet)
library(slam) # <- adding to library so we can process sparse sums
```

Now, it's time to load and preprocess the IMDB reviews (which I downloaded from Kaggle using the link in the Assignment 6 word doc).
```{r load-data}

IMDB_reviews <- read.csv("IMDB_Dataset.csv", nrows=200) 
glimpse(IMDB_reviews)

```

## Data Preprocessing 

The first thing we want to do is to convert the text in all of these URLS into a Corpus. A text corpus (plural: _corpora_) "is a large and unstructured set of texts (nowadays usually electronically stored and processed) used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory."

```{r warning=FALSE, message=FALSE, cache=FALSE}
# create a corpus from the "review" text column
myCorpus <- VCorpus(VectorSource(IMDB_reviews$review))

# check example
cat(content(myCorpus[[1]]))

# convert  to lowercase
myCorpus <- tm_map(myCorpus, content_transformer(tolower))

```

Now that we have the data in a corpus, let's do some data cleaning, by converting a bunch of special characters (e.g., **@**, **/**, **]**, **$**) to a space and by removing apostrophes.

```{r warning=FALSE, message=FALSE, cache=FALSE}
#     Defining the toSpace function
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#     Defining the remApostrophe function
remApostrophe <- content_transformer(function(x,pattern) gsub(pattern, "", x))
#     Removing special characters
myCorpus <- tm_map(myCorpus, toSpace, "@")
myCorpus <- tm_map(myCorpus, toSpace, "/")
myCorpus <- tm_map(myCorpus, toSpace, "]")
myCorpus <- tm_map(myCorpus, toSpace, "$")
myCorpus <- tm_map(myCorpus, toSpace, "—")
myCorpus <- tm_map(myCorpus, toSpace, "‐")
myCorpus <- tm_map(myCorpus, toSpace, "”")
myCorpus <- tm_map(myCorpus, toSpace, "‘")
myCorpus <- tm_map(myCorpus, toSpace, "“")
myCorpus <- tm_map(myCorpus, toSpace, "‘")
myCorpus <- tm_map(myCorpus, remApostrophe, "’")
myCorpus <- tm_map(myCorpus, remApostrophe, "«")
myCorpus <- tm_map(myCorpus, remApostrophe, "»")
```


We can again look at the first entry of the corpus. 

```{r warning=FALSE, message=FALSE, cache=FALSE}
cat(content(myCorpus[[1]]))
```

Now, let's remove numbers, punctuation, and HTML line breaks (<- Leila addition).

```{r warning=FALSE, message=FALSE, cache=FALSE}
myCorpus <- tm::tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removePunctuation)

#remove HTML line breaks ("br")
myCorpus <- tm_map(myCorpus, toSpace, "br")
myCorpus <- tm_map(myCorpus, toSpace, "<br\\s*/?>")

cat(content(myCorpus[[1]]), sep = "\n")

```

Now, let's look at a list of English stop words (e.g., _a_, _to_) that we can remove from the documents. Stop words are frequent terms that often don't provide a lot of useful information.

```{r warning=FALSE, message=FALSE, cache=FALSE}
stopwords("english")
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))
cat(content(myCorpus[[1]]), sep = "\n")
```

# commented the below chunk out b/c I don't think it's necessary for our dataset
We can also remove additional (i.e., self-defined) stop words, such as _ebook_ that appear in the first few lines of the text.

# ```{r warning=FALSE, message=FALSE, cache=FALSE}
# myCorpus <- tm_map(myCorpus, removeWords,c("ebook", "author", "translators",
#                                            "start wwwgutenbergorg", "gutenberg", "chapter", "book", "contents"))
# cat(content(myCorpus[[1]])[980:1000], sep = "\n")
# ```

# also commenting the below chunk out bc it introduced a lot of typos:
Lastly, depending on the problem, we can potentially play around with stemming. This removes common word suffixes and endings like _es_, _ed_, _ing_, etc. Alternatively, there is lemmatization, which groups together different inflected forms of the same word. Lemmatization can also be done in R.

# ```{r warning=FALSE, message=FALSE, cache=FALSE, eval=FALSE}
# myCorpus <- tm_map(myCorpus, stemDocument)
# cat(content(myCorpus[[1]]), sep = "\n")
# ```

#I've tried to lemmatize a few different ways but I keep getting errors... I think it won't work with 50k observations, and it doesn't seem to be required, so I'm skipping it for now.

## Term Document Matrix
The next (optional) step is to create a term document matrix (TDM). Technically, this step here is unnecessary, but is presented for the sake of demonstration. A TDM is a representation of how frequently different terms (shown in rows) appear in each of the documents (shown in columns). The transpose of the TDM is the document term matrix (DTM), where the rows and columns are switched.
```{r warning=FALSE, message=FALSE, cache=FALSE}
tdm <- TermDocumentMatrix(myCorpus)
tdm

```

OLD INSTRUCTIONS: Now, let's convert the TDM to a matrix that we call m. Each of the rows in m corresponds to each of the unique terms (words) that appears in the documents, and each of the columns corresponds to each document.

^These instructions didn't work with such a large dataset. Instead, 

``` {r warning=FALSE, message=FALSE, cache=FALSE}

m <- sort(row_sums(tdm), decreasing = TRUE)
head(m, 20)

```

Let's filter our data to include only those words that are actually in the Scrabble Dictionary (available through the `r` package `words`). Again, this step is not really necessary, but is shown for the sake of demonstration.

```{r warning=FALSE, message=FALSE, cache=FALSE}
dictionary <- as.character(words::words$word)

# row_names <- names(m)
# in_dictionary <- row_names %in% dictionary
# remove <- row_names[!in_dictionary]

#Since the data are so large, if we try to remove all words at once, we get an error. So we will remove them in chunks of 1000.

# num_observations <- as.numeric(length(remove))  # Total number of observations
# chunk_size <- 1000                              # Number of observations to display at a time

# for (i in seq(1, num_observations, chunk_size)) {
#   start <- i
#   end <- i + chunk_size - 1
#   end <- ifelse(end > num_observations, num_observations, end)
#   myCorpus <- tm_map(myCorpus, removeWords, remove[start:end])  
# }

#instead, trying:
#keep_terms <- names(term_freq) %in% dictionary
#tdm_filtered <- tdm[keep_terms, ]

#term_freq_filtered <- sort(row_sums(tdm_filtered), decreasing = TRUE)
#head(term_freq_filtered, 20)

```

Let's look at the terms that were dropped in myCorpus -- these are all the words that aren't in the Scrabble Dictionary (e.g., names like _Anatole_).

```{r warning=FALSE, message=FALSE, cache=FALSE}
# cat(content(myCorpus[[1]])[980:1000], sep = "\n")

#dropped_terms <- setdiff(
#  Terms(tdm),
#  Terms(tdm_filtered)
#)

#head(dropped_terms, 50)
#change here: Because dictionary filtering was applied at the term–document matrix level rather than the corpus level (to avoid recursion and memory limits in tm), I inspected removed terms directly from the TDM rather than from the corpus text.
#worth noting: when I inspected the results, it included a lot of words that should have been excluded, but also some valid words (e.g. "abysmal"). I just chose to move on + treat it as a limitation, because we don't have time to inspect the whole dataset.
```

## Document Term Matrix

ORIGINAL INSTRUCTIONS: Now that `myCorpus` has had all the words that aren't in the dictionary removed, let's convert it to a document term matrix (DTM). This is the format that we actually want - where the documents are rows, and terms are columns.

NEW METHOD, TO PREVENT ERRORS: Derive DTM from filtered TDM
```{r warning=FALSE, message=FALSE, cache=FALSE}
# dtm_cleaned <- DocumentTermMatrix(myCorpus)
# tm::inspect(dtm_cleaned)

dtm_cleaned <- tdm#_filtered
dtm_cleaned <- t(dtm_cleaned)  #transpose TDM → DTM

dtm_cleaned
```

As earlier, let's convert the DTM to a matrix.
```{r warning=FALSE, message=FALSE, cache=FALSE}
m <- dtm_cleaned  # keep as a sparse matrix
dim(m)
rownames(m) <- paste0("Review_", seq_len(nrow(m)))

# m <- as.matrix(dtm_cleaned)
# dim(m)
# colnames(m) <- dtm_cleaned$dimnames$Terms
# rownames(m) <- c("War and Peace", "Crime and Punishment", "Pride and Prejudice", "Tale of Two Cities", "Emma", "Brothers Karamazov", "Jane Eyre", 
# "The Great Gatsby", "Wuthering Heights", "Moby Dick", "Call of the Wild", "Frankenstein", "Little Women", "Vanity Fair", "Anna Karenina", 
# "Count of Monte Cristo", "Great Expectations", "Les Miserables", "Dracula", "Madame Bovary", "Don Quixote", "Gullivers Travels", "Huckleberry Finn", 
# "Picture of Dorian Gray", "White Fang", "A Christmas Carol", "The Idiot", "The Time Machine", "Age of Innocence", "Of Human Bondage", "Paradise Lost", 
# "Robinson Crusoe", "Candide", "Last of the Mohicans", "Dead Souls", "Scarlet Letter", "Treasure Island", "Ulysses", "The Trial", "The Three Musketeers", "The Metamorphosis", "Faust", "The Prince", "Leviathan", "David Copperfield", "Notre Dame de Paris", "Utopia")
```

Now, let's look at the term distribution. The histogram and tabulation can show us the distribution of term frequency across all terms. We can see that there are a lot of terms that appear only a few times, while some others appear thousands of times. The word cloud shows us the terms that appear the most, such that higher frequency is indicated by a larger font.

```{r warning=FALSE, message=FALSE, cache=FALSE}
#sparse-safe term frequency distribution:
cs <- col_sums(m)
hist(cs, breaks = 100)

tab <- table(cs)

wordcloud(
  words = names(cs),
  freq  = cs,
  min.freq = 1000,
  max.words = 150
)

# cs <- as.matrix(colSums(m))             #How many times each term appears across all documents (texts)
# rownames(cs) <- dtm_cleaned$dimnames$Terms
# 
# hist(cs, breaks=100)                    #Let's look at some histograms/tabulations/word cloud of total term appearance. 
# tab <- as.matrix(table(cs))
# wordcloud(myCorpus, min.freq=1000)
```

In our preparation for cluster analysis, let's remove all variables where the column sum is less than 10,000 (i.e., if the term appears less than 10,000 times in all documents). We are only doing it here so that we have a reasonable number of variables to include in the cluster analysis for the sake of this example. If we were to have a lot more observations, we wouldn't necessarily need or want to do this.

```{r warning=FALSE, message=FALSE, cache=FALSE}
variables_to_remove <- cs < 100 #changed to not get rid of too many

# Subset matrix frame, excluding those variables
m_subset <- m[, !variables_to_remove]
```

Let's do some additional data preparation for the cluster analysis.

```{r warning=FALSE, message=FALSE, cache=FALSE}
# #Some books are longer, others are shorter. Let's divide the frequencies by the total number of words (after processing) in each book.
# m_fin <- m_subset/rowSums(m)

# Normalize by document length
doc_lengths <- row_sums(m)
m_fin <- m_subset / doc_lengths

# Let's scale (normalize) each of the variables (relative frequency)
m_scale <- scale(as.matrix(m_fin))
```

## Text Clustering

Of course, before performing the k-means analysis on the scaled (normalized) relative frequencies of words, we need to identify the optimal number of clusters. We do this using the Scree Plot and the `NbClust` package in R.

```{r warning=FALSE, message=FALSE, cache=FALSE}
# #Scree Plot
# wss <- (nrow(m_scale)-1)*sum(apply(m_scale,2,var))
# for (i in 2:20) wss[i] <- sum(kmeans(m_scale, 
#                                      centers=i)$withinss)
# plot(1:20, wss, type="b", xlab="Number of Clusters",
#      ylab="Within groups sum of squares")
# 
# #NbClust approach
set.seed(1234)
nc <- NbClust(m_scale, min.nc=2, max.nc=15, method="kmeans", index="all")
table(nc$Best.n[1,])
par(mfrow=c(1,1)) 
barplot(table(nc$Best.n[1,]),
        xlab="Numer of Clusters", ylab="Number of Criteria",
        main="Number of Clusters Chosen by 26 Criteria")


set.seed(123)
max_k <- 15
wss <- numeric(max_k)

for (k in 1:max_k) {
  wss[k] <- kmeans(m_scale, centers = k, nstart = 20)$tot.withinss
}

plot(
  1:max_k, wss,
  type = "b",
  pch = 19,
  xlab = "Number of clusters (k)",
  ylab = "Total within-cluster sum of squares",
  main = "Elbow (Scree) Plot"
)
```

We will use 2 clusters based on the NbClust approach, since the scree plot doesn't yield a clear recommendation.

```{r warning=FALSE, message=FALSE, cache=FALSE}
k_means_results <- kmeans(m_scale, 2, 30)
```

Let's see which book falls in which cluster, as well as each cluster's size. We see that there are 17 documents (books) in cluster 1 and 30 in cluster 2.

```{r warning=FALSE, message=FALSE, cache=FALSE}
k_means_results$cluster
k_means_results$size
```

Finally, let's look at the number of times each of the terms appears in each cluster. We can also do this with proportions - that may make more sense. Ultimately, we see that certain terms have different frequencies in the clusters, but again, relative frequencies may be more relevant here.

```{r message=FALSE, warning=FALSE, cache=FALSE}
#word_totals_by_cluster <- round(aggregate(m_subset, by=list(cluster=k_means_results$cluster), sum),1)
#had to comment that out due to error, instead used:
word_totals_by_cluster <- rowsum(
  as.matrix(m_subset),
  k_means_results$cluster
)
#I got all this from chatgpt b/c Eugene's code wasn't working but idk if it's right
library(slam)

# dtm = DocumentTermMatrix
# k_means_results$cluster = cluster labels for each document

word_totals_by_cluster_sparse <- rollup(
  dtm_cleaned,                   # your DocumentTermMatrix
  MARGIN = 1,            # 1 = rows/documents
  index = k_means_results$cluster,
  FUN = sum
)

top_terms <- names(sort(col_sums(word_totals_by_cluster_sparse), decreasing = TRUE))[1:10]
plot_matrix <- as.matrix(word_totals_by_cluster_sparse[, top_terms])
rownames(plot_matrix) <- paste0("Cluster_", 1:nrow(plot_matrix))

par(cex.axis = 0.8, las = 2)

barplot(
  t(plot_matrix),
  beside = TRUE,
  col = c("blue", "green"),
  legend.text = rownames(plot_matrix),
  args.legend = list(x = "topright"),
  main = "Top Words by Cluster",
  xlab = "Words",
  ylab = "Total Frequency"
)



#Let's plot the results!
#Decrease font size and rotate x-axis labels vertically
#par(cex.axis = 0.7)  # Adjust the font size
#par(las = 2)        # Rotate labels vertically

#barplot(as.matrix(word_totals_by_cluster[-1]),
#        beside = TRUE,
#        col = c("blue", "green"),
#        legend.text = TRUE,
#        args.legend = list(x = "topright"))
#title(
#  main = "Bar Plot of Sums by Cluster",
#  xlab = "Words",
#  ylab = "Sum"
#)

# Add labels to the x-axis and y-axis. Here, the first cluster is in blue and the second one is in green.
#title(xlab = "Cluster")
#title(ylab = "Sum")

# Add a title to the plot
#title(main = "Bar Plot of Sums by Group")
```


## Sentiment Analysis

The `syuzhet` package in R has several sentiment lexicons in it. A sentiment lexicon, also known as a sentiment dictionary, is a collection of words or phrases annotated with sentiment polarity information. It associates each word or phrase with a sentiment score indicating its positive, negative, or neutral sentiment. Sentiment lexicons are commonly used in sentiment analysis tasks to determine the sentiment or emotional tone expressed in text. A lot of the words are omitted from these lexicons, because they are neutral (e.g., _hair_, _purple_, _walk_). 

The lexicons in the `syuzhet` package include:

1. NRC Lexicon: The NRC (National Research Council) lexicon is a sentiment lexicon included in the `syuzhet` package. It contains a comprehensive list of words annotated with different sentiment categories, such as positive, negative, anger, fear, joy, sadness, and more. The NRC lexicon enables sentiment analysis by associating words with specific emotional dimensions.

2. AFINN Lexicon: The AFINN lexicon is another sentiment lexicon available in `syuzhet`. It is based on a list of pre-computed sentiment scores for English words. Each word in the lexicon is assigned a score ranging from negative (-5) to positive (5), indicating its sentiment intensity. This lexicon is relatively simple and easy to use, making it popular for basic sentiment analysis tasks.

3. Bing Lexicon: The Bing lexicon, also known as the Bing Liu lexicon, is a sentiment lexicon provided by the `syuzhet` package. It consists of words categorized as either positive or negative based on their sentiment. The Bing lexicon is often used in sentiment analysis applications and can help in determining the polarity of text.

4. Syuzhet (Jockers) Lexicon: The Syuzhet lexicon is a sentiment lexicon specifically designed for the `syuzhet` package. It aims to capture sentiment by analyzing changes in the emotional intensity of a text over time. Unlike the NRC, AFINN, and Bing lexicons, the Syuzhet lexicon focuses on the temporal dynamics of sentiment, providing a unique perspective on the emotional trajectory within a piece of text. It may lend itself well to analyses presented here: https://www.tidytextmining.com/sentiment.html.

These lexicons within the `syuzhet` package offer different approaches to sentiment analysis, ranging from basic word-based scoring to more complex temporal sentiment analysis techniques. Each lexicon has its own strengths and characteristics, allowing users to choose the most suitable approach based on their specific needs and requirements.

We use sentiment analysis to examine the sentiment in an entire body of text. This is often done by aggregating (i.e., averaging or summing) sentiment scores of all the words in the text. One issue is that for longer texts, the positive and negative terms often tend to wash each other out. Therefore, shorter texts of a few sentences or paragraphs work well.

Another thing to keep in mind is that this approach doesn't take into consideration the negative terms preceding a word (e.g., _NOT good_) or sarcasm (e.g., _I'm fired? Well that's just GREAT!_). Additional data preprocessing may be necessary for this approach to work as intended, though we will skip it here. Note that removal of stop words and some of the other pre-processing done earlier is generally unnecessary for sentiment analysis, because a lot of these terms (e.g., _a_, _the_, _through_, _such_) are neutral terms and won't affect the sentiment.

Let's take a look at the four sentiment lexicons mentioned above. Here, we will print the first 20 terms of each of the lexicons.

```{r warning=FALSE, message=FALSE, cache=FALSE}
nrc <- syuzhet::get_sentiment_dictionary(dictionary="nrc")
head(nrc, n=20L)
afinn <- syuzhet::get_sentiment_dictionary(dictionary="afinn")
head(afinn, n=20L)
bing <- syuzhet::get_sentiment_dictionary(dictionary="bing")
head(bing, n=20L)
syuzhet <- syuzhet::get_sentiment_dictionary(dictionary="syuzhet")
head(syuzhet, n=20L)
```

Now, let's play around with the `nrc` lexicon. First, we can get the sentiment for any term, e.g., gorgeous.
```{r warning=FALSE, message=FALSE, cache=FALSE}
get_nrc_sentiment("gorgeous")
```

Now, let's go back to our book example. For the sake of this lexicon, let's take the first book in our data, War And Peace, which is the first row of the matrix `m`. The `get_nrc_sentiment` command obtains the sentiment score for each word in War And Peace that wasn't removed in the cleaning process above.

```{r warning=FALSE, message=FALSE, cache=FALSE}

#added some code to fix error
first_row_matrix <- as.matrix(dtm_cleaned[1, ])  # select first row
Review_1 <- data.frame(term = colnames(dtm_cleaned), count = as.numeric(first_row_matrix[1, ]))


colnames(Review_1)[2] = "Term_Frequency"
rownames(Review_1) <- 1:nrow(Review_1)

nrc_sentiment <- get_nrc_sentiment(Review_1$term)
```

Let's combine the original data frame and the sentiment counts.

```{r warning=FALSE, message=FALSE, cache=FALSE}
Review_1_Sentiment <- cbind(Review_1, nrc_sentiment)

```

Now let's multiply the sentiment by the frequency of the term.

```{r warning=FALSE, message=FALSE, cache=FALSE}
# Select the columns to be multiplied (last ten columns)
cols_to_multiply <- names(Review_1_Sentiment)[3:12] 

# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Review_1_Sentiment[, cols_to_multiply] <- Review_1_Sentiment[, cols_to_multiply] * Review_1_Sentiment$Term_Frequency
```

Now, let's see the total prevalence of each sentiment in the text by summing each column and creating a bar plot

```{r warning=FALSE, message=FALSE, cache=FALSE}
Review_1_Sentiment_Total <- t(as.matrix(colSums(Review_1_Sentiment[,-1:-2]))) 
barplot(Review_1_Sentiment_Total, las=2, ylab='Count', main='Sentiment Scores')

```

Note that this can be done for each of the books, and then the total sentiment scores (ideally, weighted by the number of terms in each book and then normalized) can be subjected to a cluster analysis. This way, we can see if we can group the books based on the sentiments expressed, rather than the most frequently occurring terms themselves. This will be skipped in this Markdown.

Now, let's play around with some of the other lexicons. Specifically, we will use the `get_sentiment` command to get the scores for each of the terms using each dictionary, and save the output to the original `War_And_Peace` data frame.

```{r warning=FALSE, message=FALSE, cache=FALSE}
#War_And_Peace$Syuzhet <- as.matrix(get_sentiment(War_And_Peace$Term, method="syuzhet"))
#hist(War_And_Peace$Syuzhet)
#War_And_Peace$Bing <- as.matrix(get_sentiment(War_And_Peace$Term, method="bing"))
#hist(War_And_Peace$Bing)
#War_And_Peace$AFINN <- as.matrix(get_sentiment(War_And_Peace$Term, method="afinn"))
#hist(War_And_Peace$AFINN)
#War_And_Peace$NRC <- as.matrix(get_sentiment(War_And_Peace$Term, method="nrc"))   #There are Negative and Positive sentiments in the NRC output above.
#hist(War_And_Peace$NRC)

```

We can compare the results from the different lexicons by assigning a value of -1 to all terms that have a negative sentiment, 0 to all terms that have a neutral sentiment, and 1 to all terms that have a positive sentiment.

```{r warning=FALSE, message=FALSE, cache=FALSE}
#sentiment_columns <- War_And_Peace[ , 3:6]
#sentiment_columns <- data.frame(lapply(sentiment_columns, sign))
#sentiment_columns <- data.frame(lapply(sentiment_columns, as.factor))
```

Now, let's see the prevalence of _unique_ negative, neutral terms (here, we are talking about _unique_ terms because we are not weighing them by how often they appear in the document).

```{r warning=FALSE, message=FALSE, cache=FALSE}
#Raw frequencies
#sapply(sentiment_columns, function(x) if("factor" %in% class(x)) {table(x)})
#Proportions
#sapply(sentiment_columns, function(x) if("factor" %in% class(x)) {prop.table(table(x))})
```

We can see that in general, most of the terms are neutral, with about 2-6% of terms being positive and 4-12% of the terms being negative, depending on which lexicon we use. 


## References

1. Hill, Chelsey. 2023. "Sentiment Analysis (Lexicons)". Rstudio-Pubs-Static.S3.Amazonaws.Com. https://rstudio-pubs-static.s3.amazonaws.com/676279_2fa8c2a7a3da4e7089e24442758e9d1b.html.

2. "Sentiment Analysis In R | R-Bloggers". 2021. R-Bloggers. https://www.r-bloggers.com/2021/05/sentiment-analysis-in-r-3/.

3. Robinson, Julia. 2023. "2 Sentiment Analysis With Tidy Data | Text Mining With R". Tidytextmining.Com. https://www.tidytextmining.com/sentiment.html.

4. "Text Mining: Sentiment Analysis · AFIT Data Science Lab R Programming Guide ". 2023. Afit-R.Github.Io. https://afit-r.github.io/sentiment_analysis.

5. "TDM (Term Document Matrix) And DTM (Document Term Matrix)". 2023. Medium. https://medium.com/analytics-vidhya/tdm-term-document-matrix-and-dtm-document-term-matrix-8b07c58957e2.

6. "Text Clustering With R: An Introduction For Data Scientists". 2018. Medium. https://medium.com/@SAPCAI/text-clustering-with-r-an-introduction-for-data-scientists-c406e7454e76.

7. "Introductory Tutorial To Text Clustering With R". 2023. Rstudio-Pubs-Static.S3.Amazonaws.Com. https://rstudio-pubs-static.s3.amazonaws.com/445820_c6663e5a79874afdae826669a9499413.html.

8. "Library Guides: Text Mining & Text Analysis: Language Corpora". 2023. Guides.Library.Uq.Edu.Au. https://guides.library.uq.edu.au/research-techniques/text-mining-analysis/language-corpora.